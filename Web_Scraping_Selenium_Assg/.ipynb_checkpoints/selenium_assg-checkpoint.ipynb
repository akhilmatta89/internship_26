{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800d582d",
   "metadata": {},
   "source": [
    "# WEB SCRAPING USING SELENIUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d2e39",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b735da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fed7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def naukri_job_search(designation,location):\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get('https://www.naukri.com/')\n",
    "    role_search = driver.find_element_by_class_name('suggestor-input ')\n",
    "    role_search.send_keys(designation)\n",
    "    location_search = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "    location_search.send_keys(location)\n",
    "    serach_path = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "    serach_path.click()\n",
    "    title = []\n",
    "    company = []\n",
    "    experience = []\n",
    "    location = []\n",
    "    title_tag = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "    for ttl in title_tag:\n",
    "        title.append(ttl.text)\n",
    "    company_tag = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[0:10]\n",
    "    for cmpny in company_tag:\n",
    "        company.append(cmpny.text)\n",
    "    exp_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]\n",
    "    \n",
    "    for exp in exp_tag:\n",
    "        experience.append(exp.text)\n",
    "    location_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[0:10]\n",
    "    for lctn in location_tag:\n",
    "        location.append(lctn.text)\n",
    "    time.sleep(5)\n",
    "    driver.close()\n",
    "    return title, company, experience, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb51a5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr Domain Expert -Data Analysts</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst (Digital Services Analytics)</td>\n",
       "      <td>Dell Technologies</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - IIM/ISB/MDI/FMS/SP Jain</td>\n",
       "      <td>K12 Techno Services Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Staff Business Data Analyst - FDP</td>\n",
       "      <td>Intuit Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Vedantu Innovations</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst 2</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Professional Data Analyst</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MDM - Data Analyst</td>\n",
       "      <td>Shell</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0                   Sr Domain Expert -Data Analysts   \n",
       "1  Senior Data Analyst (Digital Services Analytics)   \n",
       "2                               Senior Data Analyst   \n",
       "3            Data Analyst - IIM/ISB/MDI/FMS/SP Jain   \n",
       "4                 Staff Business Data Analyst - FDP   \n",
       "5                                      Data Analyst   \n",
       "6                                    Data Analyst 2   \n",
       "7                  Senior Professional Data Analyst   \n",
       "8     Data Analyst - Python/Artificial Intelligence   \n",
       "9                                MDM - Data Analyst   \n",
       "\n",
       "                       Company  \\\n",
       "0                      Siemens   \n",
       "1            Dell Technologies   \n",
       "2                        Intel   \n",
       "3  K12 Techno Services Pvt Ltd   \n",
       "4                  Intuit Inc.   \n",
       "5          Vedantu Innovations   \n",
       "6                       PayPal   \n",
       "7               DXC Technology   \n",
       "8            iMindYourBusiness   \n",
       "9                        Shell   \n",
       "\n",
       "                                            Location Experience  \n",
       "0                                Bangalore/Bengaluru   0-10 Yrs  \n",
       "1                                Bangalore/Bengaluru    1-6 Yrs  \n",
       "2                                Bangalore/Bengaluru    3-8 Yrs  \n",
       "3                                Bangalore/Bengaluru    4-9 Yrs  \n",
       "4                                Bangalore/Bengaluru    5-7 Yrs  \n",
       "5                                Bangalore/Bengaluru    0-3 Yrs  \n",
       "6                                Bangalore/Bengaluru    5-8 Yrs  \n",
       "7                                Bangalore/Bengaluru    3-7 Yrs  \n",
       "8  Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...    0-2 Yrs  \n",
       "9                                Bangalore/Bengaluru    3-4 Yrs  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, company, experience, location = naukri_job_search(designation='Data Analyst', location='Bangalore')\n",
    "Data_analyst_jobs = pd.DataFrame({})\n",
    "Data_analyst_jobs['Title']=title\n",
    "Data_analyst_jobs['Company'] = company\n",
    "Data_analyst_jobs['Location']=location\n",
    "Data_analyst_jobs['Experience']=experience\n",
    "Data_analyst_jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c0621",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74384c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>GSK India</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA Scientist with Fraud Analytics Experience</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist II- Merchandise &amp; Discovery</td>\n",
       "      <td>Swiggy</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Meesho</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist/Data Scientist</td>\n",
       "      <td>Tredence Analytics Solutions Private Limited</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Zoom Start India</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Rakuten, Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist Opportunity with PayU ( Tier1 i...</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                              Senior Data Scientist   \n",
       "2                              Junior Data Scientist   \n",
       "3     DATA Scientist with Fraud Analytics Experience   \n",
       "4         Data Scientist II- Merchandise & Discovery   \n",
       "5                           Principal Data Scientist   \n",
       "6               Senior Data Scientist/Data Scientist   \n",
       "7                              Senior Data Scientist   \n",
       "8                           Principal Data Scientist   \n",
       "9  Data Scientist Opportunity with PayU ( Tier1 i...   \n",
       "\n",
       "                                        Company  \\\n",
       "0                                      Flipkart   \n",
       "1                                     GSK India   \n",
       "2                                     Accenture   \n",
       "3                                    Concentrix   \n",
       "4                                        Swiggy   \n",
       "5                                        Meesho   \n",
       "6  Tredence Analytics Solutions Private Limited   \n",
       "7                              Zoom Start India   \n",
       "8                                 Rakuten, Inc.   \n",
       "9                                          PayU   \n",
       "\n",
       "                                            Location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                                Bangalore/Bengaluru  \n",
       "2                                Bangalore/Bengaluru  \n",
       "3                                Bangalore/Bengaluru  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5                                Bangalore/Bengaluru  \n",
       "6  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, company, experience, location = naukri_job_search(designation='Data Scientist', location='Bangalore')\n",
    "Data_scientist_jobs = pd.DataFrame({})\n",
    "Data_scientist_jobs['Title']=title\n",
    "Data_scientist_jobs['Company'] = company\n",
    "Data_scientist_jobs['Location']=location\n",
    "Data_scientist_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c247f4d",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "        You have to use the location and salary filter.\n",
    "        You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "        You have to scrape the job-title, job-location, company name, experience required.\n",
    "        The location filter to be used is ‚ÄúDelhi/NCR‚Äù. T he salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "    The task will be done as shown in the below steps:\n",
    "        1. first get the webpage https://www.naukri.com/\n",
    "        2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "        3. Then click the search button.\n",
    "        4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "        5. Then scrape the data for the first 10 jobs results you get.\n",
    "        6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1756655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>GSK India</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA Scientist with Fraud Analytics Experience</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>3,00,000 - 4,00,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist II- Merchandise &amp; Discovery</td>\n",
       "      <td>Swiggy</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Meesho</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Rakuten, Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist Opportunity with PayU ( Tier1 i...</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist 1</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist/Data Scientist</td>\n",
       "      <td>Tredence Analytics Solutions Private Limited</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                              Junior Data Scientist   \n",
       "2                              Senior Data Scientist   \n",
       "3     DATA Scientist with Fraud Analytics Experience   \n",
       "4         Data Scientist II- Merchandise & Discovery   \n",
       "5                           Principal Data Scientist   \n",
       "6                           Principal Data Scientist   \n",
       "7  Data Scientist Opportunity with PayU ( Tier1 i...   \n",
       "8                                   Data Scientist 1   \n",
       "9               Senior Data Scientist/Data Scientist   \n",
       "\n",
       "                                        Company  \\\n",
       "0                                      Flipkart   \n",
       "1                                     Accenture   \n",
       "2                                     GSK India   \n",
       "3                                    Concentrix   \n",
       "4                                        Swiggy   \n",
       "5                                        Meesho   \n",
       "6                                 Rakuten, Inc.   \n",
       "7                                          PayU   \n",
       "8                                        PayPal   \n",
       "9  Tredence Analytics Solutions Private Limited   \n",
       "\n",
       "                                            Location Experience  \\\n",
       "0                                Bangalore/Bengaluru    5-8 Yrs   \n",
       "1                                Bangalore/Bengaluru    2-6 Yrs   \n",
       "2                                Bangalore/Bengaluru    5-9 Yrs   \n",
       "3                                Bangalore/Bengaluru    2-4 Yrs   \n",
       "4                                Bangalore/Bengaluru    2-5 Yrs   \n",
       "5                                Bangalore/Bengaluru   6-10 Yrs   \n",
       "6                                Bangalore/Bengaluru   7-12 Yrs   \n",
       "7  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...    2-5 Yrs   \n",
       "8                                Bangalore/Bengaluru    4-8 Yrs   \n",
       "9  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...    2-7 Yrs   \n",
       "\n",
       "                    Salary  \n",
       "0            Not disclosed  \n",
       "1            Not disclosed  \n",
       "2            Not disclosed  \n",
       "3  3,00,000 - 4,00,000 PA.  \n",
       "4            Not disclosed  \n",
       "5            Not disclosed  \n",
       "6            Not disclosed  \n",
       "7            Not disclosed  \n",
       "8            Not disclosed  \n",
       "9            Not disclosed  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "driver2 = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver2.get('https://www.naukri.com/')\n",
    "role_search = driver2.find_element_by_class_name('suggestor-input ')\n",
    "role_search.send_keys('Data Scientist')\n",
    "serach_path = driver2.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "serach_path.click()\n",
    "time.sleep(3)\n",
    "#finding the bangalore check box\n",
    "loc=driver2.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/i\")\n",
    "loc.click()\n",
    "time.sleep(2)\n",
    "# finding the salary check box\n",
    "slry_box = driver2.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[1]/div[2]/div[1]/label/i')\n",
    "\n",
    "#//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\n",
    "slry_box.click()\n",
    "time.sleep(3)\n",
    "title = []\n",
    "company = []\n",
    "experience = []\n",
    "location = []\n",
    "salary = []\n",
    "title_tag = driver2.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "for ttl in title_tag:\n",
    "    title.append(ttl.text)\n",
    "company_tag = driver2.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[0:10]\n",
    "for cmpny in company_tag:\n",
    "    company.append(cmpny.text)\n",
    "exp_tag = driver2.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]\n",
    "for exp in exp_tag:\n",
    "    experience.append(exp.text)\n",
    "location_tag = driver2.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[0:10]\n",
    "for lctn in location_tag:\n",
    "    location.append(lctn.text)\n",
    "sal_tag = driver2.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi salary']/span[1]\")[0:10]\n",
    "for sal in sal_tag:\n",
    "    salary.append(sal.text)\n",
    "driver2.close()\n",
    "Data_Scientist_filtered_jobs = pd.DataFrame({})\n",
    "Data_Scientist_filtered_jobs['Title']=title\n",
    "Data_Scientist_filtered_jobs['Company'] = company\n",
    "Data_Scientist_filtered_jobs['Location']=location\n",
    "Data_Scientist_filtered_jobs['Experience']=experience\n",
    "Data_Scientist_filtered_jobs['Salary']=salary\n",
    "Data_Scientist_filtered_jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30c46d",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands andmore‚Äù is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom ofthe page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33550e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Gradient, Toughened Glass Lens, UV Protection ...</td>\n",
       "      <td>‚Çπ614</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (59)</td>\n",
       "      <td>‚Çπ599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ148</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ709</td>\n",
       "      <td>21% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>‚Çπ208</td>\n",
       "      <td>91% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Rich Club</td>\n",
       "      <td>UV Protection, Polarized Round Sunglasses (50)</td>\n",
       "      <td>‚Çπ399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ113</td>\n",
       "      <td>92% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PHENOMENAL</td>\n",
       "      <td>UV Protection, Mirrored Clubmaster Sunglasses ...</td>\n",
       "      <td>‚Çπ319</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kingsunglasses</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (55)</td>\n",
       "      <td>‚Çπ183</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (50)</td>\n",
       "      <td>‚Çπ599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                   Item Description  Cost  \\\n",
       "0     Singco India  Gradient, Toughened Glass Lens, UV Protection ...  ‚Çπ614   \n",
       "1    VINCENT CHASE             UV Protection Wayfarer Sunglasses (59)  ‚Çπ599   \n",
       "2             SRPM             UV Protection Wayfarer Sunglasses (50)  ‚Çπ148   \n",
       "3         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)  ‚Çπ709   \n",
       "4        Elligator                UV Protection Round Sunglasses (54)  ‚Çπ208   \n",
       "..             ...                                                ...   ...   \n",
       "95       Rich Club     UV Protection, Polarized Round Sunglasses (50)  ‚Çπ399   \n",
       "96       New Specs   UV Protection Rectangular Sunglasses (Free Size)  ‚Çπ113   \n",
       "97      PHENOMENAL  UV Protection, Mirrored Clubmaster Sunglasses ...  ‚Çπ319   \n",
       "98  kingsunglasses          UV Protection Rectangular Sunglasses (55)  ‚Çπ183   \n",
       "99   VINCENT CHASE          UV Protection Rectangular Sunglasses (50)  ‚Çπ599   \n",
       "\n",
       "   Discount  \n",
       "0   79% off  \n",
       "1   70% off  \n",
       "2   88% off  \n",
       "3   21% off  \n",
       "4   91% off  \n",
       "..      ...  \n",
       "95  60% off  \n",
       "96  92% off  \n",
       "97  84% off  \n",
       "98  87% off  \n",
       "99  70% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver.get('https://www.flipkart.com/')\n",
    "driver.find_element_by_xpath('/html/body/div[2]/div/div/button').click()\n",
    "item_search = driver.find_element_by_class_name('_3704LK')\n",
    "item_search.send_keys('sunglasses')\n",
    "button= driver.find_element_by_class_name('_34RNph')\n",
    "button.click()\n",
    "time.sleep(3)\n",
    "brand=[]\n",
    "description=[]\n",
    "cost = []\n",
    "discount = []\n",
    "for rot in range(0,3):\n",
    "    brands=driver.find_elements_by_class_name('_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the brand extracted\n",
    "    description_tag = driver.find_elements_by_class_name('IRpwTa')#scraping brands name by class name='IRpwTa'\n",
    "    for i in description_tag:\n",
    "        description.append(i.text)#appending the description of sunglasses extracted\n",
    "    cost_tag = driver.find_elements_by_class_name('_30jeq3')[0:40]\n",
    "    #scraping brands name by class name='_30jeq3'\n",
    "    #i have scraped from 0:40 because the webpage has some suggestion below with same class name under Reviews for Popular Sunglasses\n",
    "    for i in cost_tag:\n",
    "        cost.append(i.text)#appending the cost of sunglasses extracted\n",
    "    discount_tag = driver.find_elements_by_class_name('_3Ay6Sb')[0:40]\n",
    "    for i in discount_tag:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "    time.sleep(2)\n",
    "driver.close()\n",
    "flipkart_sunglasses_data = pd.DataFrame({})\n",
    "flipkart_sunglasses_data['Brand']=brand[0:100]\n",
    "flipkart_sunglasses_data['Item Description'] = description[0:100]\n",
    "flipkart_sunglasses_data['Cost']=cost[0:100]\n",
    "flipkart_sunglasses_data['Discount']=discount[0:100]\n",
    "flipkart_sunglasses_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58718af9",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage \n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e68536c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating (In Stars)</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Comment Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Pretty good</td>\n",
       "      <td>I've used this phone for over a month now and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Excellent camera üì∏ And Display touching very N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>I purchased the iPhone 11 a month back. I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>The ultimate performance\\nCamera is superb\\nTh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating (In Stars)                Comment  \\\n",
       "0                  5              Brilliant   \n",
       "1                  5         Simply awesome   \n",
       "2                  5    Best in the market!   \n",
       "3                  5       Perfect product!   \n",
       "4                  5              Fabulous!   \n",
       "..               ...                    ...   \n",
       "95                 4            Pretty good   \n",
       "96                 5      Worth every penny   \n",
       "97                 5  Mind-blowing purchase   \n",
       "98                 5              Fabulous!   \n",
       "99                 5              Just wow!   \n",
       "\n",
       "                                  Comment Description  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  I've used this phone for over a month now and ...  \n",
       "96  Undoubtedly Iphone 11 is the most successful m...  \n",
       "97  Excellent camera üì∏ And Display touching very N...  \n",
       "98  I purchased the iPhone 11 a month back. I must...  \n",
       "99  The ultimate performance\\nCamera is superb\\nTh...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace')\n",
    "time.sleep(3)\n",
    "driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/a/div/span').click()\n",
    "ratings = []\n",
    "comment = []\n",
    "comment_description=[]\n",
    "time.sleep(3)\n",
    "for each_page in range(0,10):\n",
    "    rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for i in rating_tag:\n",
    "        ratings.append(i.text)\n",
    "    least_rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1rdVr6 _1BLPMq']\")\n",
    "    for i in least_rating_tag:\n",
    "        ratings.append(i.text)\n",
    "    comment_tag = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for i in comment_tag:\n",
    "        comment.append(i.text)\n",
    "    comment_description_tag = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for i in comment_description_tag:\n",
    "        comment_description.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "    time.sleep(2)\n",
    "driver.close()\n",
    "flipkart_iphone11_review_data = pd.DataFrame({})\n",
    "flipkart_iphone11_review_data['Rating (In Stars)']=ratings\n",
    "flipkart_iphone11_review_data['Comment'] = comment\n",
    "flipkart_iphone11_review_data['Comment Description']=comment_description\n",
    "flipkart_iphone11_review_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2fb56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
