{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800d582d",
   "metadata": {},
   "source": [
    "# WEB SCRAPING USING SELENIUM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d2e39",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b735da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIES\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fed7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def naukri_job_search(designation,location):\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get('https://www.naukri.com/')\n",
    "    role_search = driver.find_element_by_class_name('suggestor-input ')\n",
    "    role_search.send_keys(designation)\n",
    "    location_search = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input')\n",
    "    location_search.send_keys(location)\n",
    "    serach_path = driver.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "    serach_path.click()\n",
    "    title = []\n",
    "    company = []\n",
    "    experience = []\n",
    "    location = []\n",
    "    title_tag = driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "    for ttl in title_tag:\n",
    "        title.append(ttl.text)\n",
    "    company_tag = driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[0:10]\n",
    "    for cmpny in company_tag:\n",
    "        company.append(cmpny.text)\n",
    "    exp_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]\n",
    "    \n",
    "    for exp in exp_tag:\n",
    "        experience.append(exp.text)\n",
    "    location_tag = driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[0:10]\n",
    "    for lctn in location_tag:\n",
    "        location.append(lctn.text)\n",
    "    time.sleep(5)\n",
    "    driver.close()\n",
    "    return title, company, experience, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb51a5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr Domain Expert -Data Analysts</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Analyst (Digital Services Analytics)</td>\n",
       "      <td>Dell Technologies</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Intel</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - IIM/ISB/MDI/FMS/SP Jain</td>\n",
       "      <td>K12 Techno Services Pvt Ltd</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Staff Business Data Analyst - FDP</td>\n",
       "      <td>Intuit Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Vedantu Innovations</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst 2</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Professional Data Analyst</td>\n",
       "      <td>DXC Technology</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MDM - Data Analyst</td>\n",
       "      <td>Shell</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>3-4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0                   Sr Domain Expert -Data Analysts   \n",
       "1  Senior Data Analyst (Digital Services Analytics)   \n",
       "2                               Senior Data Analyst   \n",
       "3            Data Analyst - IIM/ISB/MDI/FMS/SP Jain   \n",
       "4                 Staff Business Data Analyst - FDP   \n",
       "5                                      Data Analyst   \n",
       "6                                    Data Analyst 2   \n",
       "7                  Senior Professional Data Analyst   \n",
       "8     Data Analyst - Python/Artificial Intelligence   \n",
       "9                                MDM - Data Analyst   \n",
       "\n",
       "                       Company  \\\n",
       "0                      Siemens   \n",
       "1            Dell Technologies   \n",
       "2                        Intel   \n",
       "3  K12 Techno Services Pvt Ltd   \n",
       "4                  Intuit Inc.   \n",
       "5          Vedantu Innovations   \n",
       "6                       PayPal   \n",
       "7               DXC Technology   \n",
       "8            iMindYourBusiness   \n",
       "9                        Shell   \n",
       "\n",
       "                                            Location Experience  \n",
       "0                                Bangalore/Bengaluru   0-10 Yrs  \n",
       "1                                Bangalore/Bengaluru    1-6 Yrs  \n",
       "2                                Bangalore/Bengaluru    3-8 Yrs  \n",
       "3                                Bangalore/Bengaluru    4-9 Yrs  \n",
       "4                                Bangalore/Bengaluru    5-7 Yrs  \n",
       "5                                Bangalore/Bengaluru    0-3 Yrs  \n",
       "6                                Bangalore/Bengaluru    5-8 Yrs  \n",
       "7                                Bangalore/Bengaluru    3-7 Yrs  \n",
       "8  Kolkata, Mumbai, Visakhapatnam, Hyderabad/Secu...    0-2 Yrs  \n",
       "9                                Bangalore/Bengaluru    3-4 Yrs  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, company, experience, location = naukri_job_search(designation='Data Analyst', location='Bangalore')\n",
    "Data_analyst_jobs = pd.DataFrame({})\n",
    "Data_analyst_jobs['Title']=title\n",
    "Data_analyst_jobs['Company'] = company\n",
    "Data_analyst_jobs['Location']=location\n",
    "Data_analyst_jobs['Experience']=experience\n",
    "Data_analyst_jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c0621",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74384c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>GSK India</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA Scientist with Fraud Analytics Experience</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist II- Merchandise &amp; Discovery</td>\n",
       "      <td>Swiggy</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Meesho</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist/Data Scientist</td>\n",
       "      <td>Tredence Analytics Solutions Private Limited</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Zoom Start India</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Rakuten, Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist Opportunity with PayU ( Tier1 i...</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                              Senior Data Scientist   \n",
       "2                              Junior Data Scientist   \n",
       "3     DATA Scientist with Fraud Analytics Experience   \n",
       "4         Data Scientist II- Merchandise & Discovery   \n",
       "5                           Principal Data Scientist   \n",
       "6               Senior Data Scientist/Data Scientist   \n",
       "7                              Senior Data Scientist   \n",
       "8                           Principal Data Scientist   \n",
       "9  Data Scientist Opportunity with PayU ( Tier1 i...   \n",
       "\n",
       "                                        Company  \\\n",
       "0                                      Flipkart   \n",
       "1                                     GSK India   \n",
       "2                                     Accenture   \n",
       "3                                    Concentrix   \n",
       "4                                        Swiggy   \n",
       "5                                        Meesho   \n",
       "6  Tredence Analytics Solutions Private Limited   \n",
       "7                              Zoom Start India   \n",
       "8                                 Rakuten, Inc.   \n",
       "9                                          PayU   \n",
       "\n",
       "                                            Location  \n",
       "0                                Bangalore/Bengaluru  \n",
       "1                                Bangalore/Bengaluru  \n",
       "2                                Bangalore/Bengaluru  \n",
       "3                                Bangalore/Bengaluru  \n",
       "4                                Bangalore/Bengaluru  \n",
       "5                                Bangalore/Bengaluru  \n",
       "6  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...  \n",
       "7                                Bangalore/Bengaluru  \n",
       "8                                Bangalore/Bengaluru  \n",
       "9  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, company, experience, location = naukri_job_search(designation='Data Scientist', location='Bangalore')\n",
    "Data_scientist_jobs = pd.DataFrame({})\n",
    "Data_scientist_jobs['Title']=title\n",
    "Data_scientist_jobs['Company'] = company\n",
    "Data_scientist_jobs['Location']=location\n",
    "Data_scientist_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c247f4d",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "        You have to use the location and salary filter.\n",
    "        You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "        You have to scrape the job-title, job-location, company name, experience required.\n",
    "        The location filter to be used is ‚ÄúDelhi/NCR‚Äù. T he salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "    The task will be done as shown in the below steps:\n",
    "        1. first get the webpage https://www.naukri.com/\n",
    "        2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "        3. Then click the search button.\n",
    "        4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "        5. Then scrape the data for the first 10 jobs results you get.\n",
    "        6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1756655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>GSK India</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA Scientist with Fraud Analytics Experience</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>3,00,000 - 4,00,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist II- Merchandise &amp; Discovery</td>\n",
       "      <td>Swiggy</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Meesho</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>Rakuten, Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist Opportunity with PayU ( Tier1 i...</td>\n",
       "      <td>PayU</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist 1</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist/Data Scientist</td>\n",
       "      <td>Tredence Analytics Solutions Private Limited</td>\n",
       "      <td>Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                              Senior Data Scientist   \n",
       "1                              Junior Data Scientist   \n",
       "2                              Senior Data Scientist   \n",
       "3     DATA Scientist with Fraud Analytics Experience   \n",
       "4         Data Scientist II- Merchandise & Discovery   \n",
       "5                           Principal Data Scientist   \n",
       "6                           Principal Data Scientist   \n",
       "7  Data Scientist Opportunity with PayU ( Tier1 i...   \n",
       "8                                   Data Scientist 1   \n",
       "9               Senior Data Scientist/Data Scientist   \n",
       "\n",
       "                                        Company  \\\n",
       "0                                      Flipkart   \n",
       "1                                     Accenture   \n",
       "2                                     GSK India   \n",
       "3                                    Concentrix   \n",
       "4                                        Swiggy   \n",
       "5                                        Meesho   \n",
       "6                                 Rakuten, Inc.   \n",
       "7                                          PayU   \n",
       "8                                        PayPal   \n",
       "9  Tredence Analytics Solutions Private Limited   \n",
       "\n",
       "                                            Location Experience  \\\n",
       "0                                Bangalore/Bengaluru    5-8 Yrs   \n",
       "1                                Bangalore/Bengaluru    2-6 Yrs   \n",
       "2                                Bangalore/Bengaluru    5-9 Yrs   \n",
       "3                                Bangalore/Bengaluru    2-4 Yrs   \n",
       "4                                Bangalore/Bengaluru    2-5 Yrs   \n",
       "5                                Bangalore/Bengaluru   6-10 Yrs   \n",
       "6                                Bangalore/Bengaluru   7-12 Yrs   \n",
       "7  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...    2-5 Yrs   \n",
       "8                                Bangalore/Bengaluru    4-8 Yrs   \n",
       "9  Pune, Gurgaon/Gurugram, Chennai, Bangalore/Ben...    2-7 Yrs   \n",
       "\n",
       "                    Salary  \n",
       "0            Not disclosed  \n",
       "1            Not disclosed  \n",
       "2            Not disclosed  \n",
       "3  3,00,000 - 4,00,000 PA.  \n",
       "4            Not disclosed  \n",
       "5            Not disclosed  \n",
       "6            Not disclosed  \n",
       "7            Not disclosed  \n",
       "8            Not disclosed  \n",
       "9            Not disclosed  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "driver2 = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver2.get('https://www.naukri.com/')\n",
    "role_search = driver2.find_element_by_class_name('suggestor-input ')\n",
    "role_search.send_keys('Data Scientist')\n",
    "serach_path = driver2.find_element_by_xpath('/html/body/div/div[2]/div[3]/div/div/div[6]')\n",
    "serach_path.click()\n",
    "time.sleep(3)\n",
    "#finding the bangalore check box\n",
    "loc=driver2.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/i\")\n",
    "loc.click()\n",
    "time.sleep(2)\n",
    "# finding the salary check box\n",
    "slry_box = driver2.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[1]/div[2]/div[1]/label/i')\n",
    "\n",
    "#//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\n",
    "slry_box.click()\n",
    "time.sleep(3)\n",
    "title = []\n",
    "company = []\n",
    "experience = []\n",
    "location = []\n",
    "salary = []\n",
    "title_tag = driver2.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "for ttl in title_tag:\n",
    "    title.append(ttl.text)\n",
    "company_tag = driver2.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")[0:10]\n",
    "for cmpny in company_tag:\n",
    "    company.append(cmpny.text)\n",
    "exp_tag = driver2.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0:10]\n",
    "for exp in exp_tag:\n",
    "    experience.append(exp.text)\n",
    "location_tag = driver2.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")[0:10]\n",
    "for lctn in location_tag:\n",
    "    location.append(lctn.text)\n",
    "sal_tag = driver2.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi salary']/span[1]\")[0:10]\n",
    "for sal in sal_tag:\n",
    "    salary.append(sal.text)\n",
    "driver2.close()\n",
    "Data_Scientist_filtered_jobs = pd.DataFrame({})\n",
    "Data_Scientist_filtered_jobs['Title']=title\n",
    "Data_Scientist_filtered_jobs['Company'] = company\n",
    "Data_Scientist_filtered_jobs['Location']=location\n",
    "Data_Scientist_filtered_jobs['Experience']=experience\n",
    "Data_Scientist_filtered_jobs['Salary']=salary\n",
    "Data_Scientist_filtered_jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca51511",
   "metadata": {},
   "source": [
    "# Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands andmore‚Äù is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom ofthe page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12848e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flipkart_item_search(search_item):\n",
    "    driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "    driver.get('https://www.flipkart.com/')\n",
    "    driver.find_element_by_xpath('/html/body/div[2]/div/div/button').click()\n",
    "    item_search = driver.find_element_by_class_name('_3704LK')\n",
    "    item_search.send_keys(search_item)\n",
    "    button= driver.find_element_by_class_name('_34RNph')\n",
    "    button.click()\n",
    "    time.sleep(3)\n",
    "    brand=[]\n",
    "    description=[]\n",
    "    cost = []\n",
    "    discount = []\n",
    "    for rot in range(0,3):\n",
    "        brands=driver.find_elements_by_class_name('_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "        for i in brands:\n",
    "            brand.append(i.text)#appending the brand extracted\n",
    "        description_tag = driver.find_elements_by_class_name('IRpwTa')#scraping brands name by class name='IRpwTa'\n",
    "        for i in description_tag:\n",
    "            description.append(i.text)#appending the description of sunglasses extracted\n",
    "        cost_tag = driver.find_elements_by_class_name('_30jeq3')[0:40]\n",
    "        #scraping brands name by class name='_30jeq3'\n",
    "        #i have scraped from 0:40 because the webpage has some suggestion below with same class name under Reviews for Popular Sunglasses\n",
    "        for i in cost_tag:\n",
    "            cost.append(i.text)#appending the cost of sunglasses extracted\n",
    "        discount_tag = driver.find_elements_by_class_name('_3Ay6Sb')[0:40]\n",
    "        for i in discount_tag:\n",
    "            discount.append(i.text)\n",
    "        nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "        except:\n",
    "            driver.get(nxt_button[0].get_attribute('href'))\n",
    "        time.sleep(2)\n",
    "    driver.close()\n",
    "    return brand, description, cost, discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85deb638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john jacobs</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (47)</td>\n",
       "      <td>‚Çπ3,850</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (59)</td>\n",
       "      <td>‚Çπ599</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ199</td>\n",
       "      <td>84% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ709</td>\n",
       "      <td>21% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>‚Çπ279</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (54)</td>\n",
       "      <td>‚Çπ175</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ253</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ348</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Sports Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ749</td>\n",
       "      <td>16% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>‚Çπ329</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                   Item Description    Cost  \\\n",
       "0      john jacobs           UV Protection Clubmaster Sunglasses (47)  ‚Çπ3,850   \n",
       "1    VINCENT CHASE             UV Protection Wayfarer Sunglasses (59)    ‚Çπ599   \n",
       "2             SRPM             UV Protection Wayfarer Sunglasses (50)    ‚Çπ199   \n",
       "3         Fastrack      UV Protection Wayfarer Sunglasses (Free Size)    ‚Çπ709   \n",
       "4        Elligator                UV Protection Round Sunglasses (54)    ‚Çπ279   \n",
       "..             ...                                                ...     ...   \n",
       "95          PIRASO              UV Protection Aviator Sunglasses (54)    ‚Çπ175   \n",
       "96  ROZZETTA CRAFT   UV Protection Rectangular Sunglasses (Free Size)    ‚Çπ253   \n",
       "97  ROZZETTA CRAFT         UV Protection Round Sunglasses (Free Size)    ‚Çπ348   \n",
       "98        Fastrack        UV Protection Sports Sunglasses (Free Size)    ‚Çπ749   \n",
       "99  ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...    ‚Çπ329   \n",
       "\n",
       "   Discount  \n",
       "0   35% off  \n",
       "1   70% off  \n",
       "2   84% off  \n",
       "3   21% off  \n",
       "4   88% off  \n",
       "..      ...  \n",
       "95  89% off  \n",
       "96  83% off  \n",
       "97  82% off  \n",
       "98  16% off  \n",
       "99  83% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand, description, cost, discount = flipkart_item_search(search_item='sunglasses')\n",
    "flipkart_sunglasses_data = pd.DataFrame({})\n",
    "flipkart_sunglasses_data['Brand']=brand[0:100]\n",
    "flipkart_sunglasses_data['Item Description'] = description[0:100]\n",
    "flipkart_sunglasses_data['Cost']=cost[0:100]\n",
    "flipkart_sunglasses_data['Discount']=discount[0:100]\n",
    "flipkart_sunglasses_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e9d9c",
   "metadata": {},
   "source": [
    "# Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage \n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3e7354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating (In Stars)</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Comment Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Great iPhone very snappy experience as apple k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Pretty good</td>\n",
       "      <td>I've used this phone for over a month now and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Undoubtedly Iphone 11 is the most successful m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Excellent camera üì∏ And Display touching very N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>I purchased the iPhone 11 a month back. I must...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>The ultimate performance\\nCamera is superb\\nTh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating (In Stars)                Comment  \\\n",
       "0                  5              Brilliant   \n",
       "1                  5         Simply awesome   \n",
       "2                  5    Best in the market!   \n",
       "3                  5       Perfect product!   \n",
       "4                  5              Fabulous!   \n",
       "..               ...                    ...   \n",
       "95                 4            Pretty good   \n",
       "96                 5      Worth every penny   \n",
       "97                 5  Mind-blowing purchase   \n",
       "98                 5              Fabulous!   \n",
       "99                 5              Just wow!   \n",
       "\n",
       "                                  Comment Description  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Really satisfied with the Product I received.....  \n",
       "2   Great iPhone very snappy experience as apple k...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   This is my first iOS phone. I am very happy wi...  \n",
       "..                                                ...  \n",
       "95  I've used this phone for over a month now and ...  \n",
       "96  Undoubtedly Iphone 11 is the most successful m...  \n",
       "97  Excellent camera üì∏ And Display touching very N...  \n",
       "98  I purchased the iPhone 11 a month back. I must...  \n",
       "99  The ultimate performance\\nCamera is superb\\nTh...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "chrome_options = Options()\n",
    "driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver.get('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC TSVZAXUHGREPBFGI&marketplace')\n",
    "time.sleep(3)\n",
    "driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/a/div/span').click()\n",
    "ratings = []\n",
    "comment = []\n",
    "comment_description=[]\n",
    "time.sleep(3)\n",
    "for each_page in range(0,10):\n",
    "    rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    for i in rating_tag:\n",
    "        ratings.append(i.text)\n",
    "    least_rating_tag = driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1rdVr6 _1BLPMq']\")\n",
    "    for i in least_rating_tag:\n",
    "        ratings.append(i.text)\n",
    "    comment_tag = driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    for i in comment_tag:\n",
    "        comment.append(i.text)\n",
    "    comment_description_tag = driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for i in comment_description_tag:\n",
    "        comment_description.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "    time.sleep(2)\n",
    "driver.close()\n",
    "flipkart_iphone11_review_data = pd.DataFrame({})\n",
    "flipkart_iphone11_review_data['Rating (In Stars)']=ratings\n",
    "flipkart_iphone11_review_data['Comment'] = comment\n",
    "flipkart_iphone11_review_data['Comment Description']=comment_description\n",
    "flipkart_iphone11_review_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988fa96",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for ‚Äúsneakers‚Äù in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19a41b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Item Description</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HIGHLANDER</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ945</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shoes Icon</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Super Stylish &amp; Trendy Combo Pack of 02 Pairs ...</td>\n",
       "      <td>‚Çπ469</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ374</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASTEROID</td>\n",
       "      <td>Original Luxury Branded Fashionable Men's Casu...</td>\n",
       "      <td>‚Çπ424</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>FEWROK</td>\n",
       "      <td>Premium quality Sneakers For Men</td>\n",
       "      <td>‚Çπ399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>White Sneakers Sneakers For Men</td>\n",
       "      <td>‚Çπ351</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack Of 2 Latest Stylish Casual Shoes fo...</td>\n",
       "      <td>‚Çπ482</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>DUCATI</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ1,499</td>\n",
       "      <td>58% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Shoes Icon</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Brand                                   Item Description    Cost  \\\n",
       "0   HIGHLANDER                                   Sneakers For Men    ‚Çπ945   \n",
       "1   Shoes Icon                                   Sneakers For Men    ‚Çπ399   \n",
       "2       Chevit  Super Stylish & Trendy Combo Pack of 02 Pairs ...    ‚Çπ469   \n",
       "3     Magnolia                                   Sneakers For Men    ‚Çπ374   \n",
       "4     ASTEROID  Original Luxury Branded Fashionable Men's Casu...    ‚Çπ424   \n",
       "..         ...                                                ...     ...   \n",
       "95      FEWROK                   Premium quality Sneakers For Men    ‚Çπ399   \n",
       "96    Magnolia                    White Sneakers Sneakers For Men    ‚Çπ351   \n",
       "97      BRUTON  Combo Pack Of 2 Latest Stylish Casual Shoes fo...    ‚Çπ482   \n",
       "98      DUCATI                                   Sneakers For Men  ‚Çπ1,499   \n",
       "99  Shoes Icon                                   Sneakers For Men    ‚Çπ399   \n",
       "\n",
       "   Discount  \n",
       "0   52% off  \n",
       "1   60% off  \n",
       "2   70% off  \n",
       "3   62% off  \n",
       "4   78% off  \n",
       "..      ...  \n",
       "95  60% off  \n",
       "96  64% off  \n",
       "97  80% off  \n",
       "98  58% off  \n",
       "99  60% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand, description, cost, discount = flipkart_item_search(search_item='sneakers')\n",
    "flipkart_sneakers_data = pd.DataFrame({})\n",
    "flipkart_sneakers_data['Brand']=brand[0:100]\n",
    "flipkart_sneakers_data['Item Description'] = description[0:100]\n",
    "flipkart_sneakers_data['Cost']=cost[0:100]\n",
    "flipkart_sneakers_data['Discount']=discount[0:100]\n",
    "flipkart_sneakers_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc39f4",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to ‚ÄúRs. 7149 to Rs. 14099 ‚Äù , Color filter to ‚ÄúBlack‚Äù, as shown inthe below image.\n",
    "And then scrape First 100 shoes data you get. The data should include ‚ÄúBrand‚Äù of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ac45eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Leather Loafers</td>\n",
       "      <td>Rs. 11199Rs. 15999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Leather Driving Shoes</td>\n",
       "      <td>Rs. 9099Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Rs. 8099Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men Dropset Training Shoes</td>\n",
       "      <td>Rs. 8449Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Printed Sneakers</td>\n",
       "      <td>Rs. 9099Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ASICS</td>\n",
       "      <td>Men Colourblocked PU Sneakers</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Wedge Sandals</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Leather Slim Heels</td>\n",
       "      <td>Rs. 7199Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Embellished Wedge Sandals</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>Men SNOWTREKKER Trekking Shoes</td>\n",
       "      <td>Rs. 11999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Product                     Description               Price\n",
       "0       ALDO             Men Leather Loafers  Rs. 11199Rs. 15999\n",
       "1       ALDO       Men Leather Driving Shoes   Rs. 9099Rs. 12999\n",
       "2   Skechers      Men Max Cushioning Running    Rs. 8099Rs. 8999\n",
       "3     ADIDAS      Men Dropset Training Shoes   Rs. 8449Rs. 12999\n",
       "4       ALDO            Men Printed Sneakers   Rs. 9099Rs. 12999\n",
       "..       ...                             ...                 ...\n",
       "95     ASICS   Men Colourblocked PU Sneakers            Rs. 9999\n",
       "96      ALDO                   Wedge Sandals            Rs. 7999\n",
       "97      ALDO              Leather Slim Heels   Rs. 7199Rs. 11999\n",
       "98      ALDO       Embellished Wedge Sandals            Rs. 7999\n",
       "99  Columbia  Men SNOWTREKKER Trekking Shoes           Rs. 11999\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver.get(' https://www.myntra.com/shoes')\n",
    "driver.find_element_by_xpath('//*[@id=\"mountRoot\"]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div').click()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div').click()\n",
    "time.sleep(2)\n",
    "product_name = []\n",
    "product_desc =[]\n",
    "product_price = []\n",
    "for pages in range(0,2):\n",
    "    product_name_tag = driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "    for i in product_name_tag:\n",
    "        product_name.append(i.text)\n",
    "    product_desc_tag = driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "    for i in product_desc_tag:\n",
    "        product_desc.append(i.text)\n",
    "    product_price_tag = driver.find_elements_by_xpath(\"//div[@class='product-price']/span[1]\")\n",
    "    for i in product_price_tag:\n",
    "        product_price.append(i.text)   \n",
    "    driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[12]/a').click()\n",
    "    time.sleep(2)\n",
    "driver.close()\n",
    "myntra_shoe_data = pd.DataFrame({})\n",
    "myntra_shoe_data['Product']=product_name\n",
    "myntra_shoe_data['Description'] = product_desc\n",
    "myntra_shoe_data['Price']=product_price\n",
    "myntra_shoe_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a4239",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon.\n",
    "Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù and ‚ÄúIntel Core i9‚Äù as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c173b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "driver.get('https://www.amazon.in/')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e55d63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rating(item):\n",
    "    try:\n",
    "        rating = item.i.text\n",
    "    except AttributeError:\n",
    "        rating = ' '\n",
    "    return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c56bb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LG Gram 14 inches Ultra-Light Intel Evo 11th G...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>‚Çπ88,499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>‚Çπ93,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>‚Çπ86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>‚Çπ57,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "      <td>‚Çπ86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LG Gram 16 inches Intel Evo 11th Gen Core i7 U...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>‚Çπ89,611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dell Alienware x17 R1 (2021) i7-11800H Gaming ...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>‚Çπ3,25,428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP Pavilion x360 11th Gen Intel Core i7 14 inc...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>‚Çπ86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS TUF Gaming F15 (2021) 15.6-inch (39.62 cm...</td>\n",
       "      <td>4.5 out of 5 stars</td>\n",
       "      <td>‚Çπ92,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LG Gram Intel Evo 11th Gen Core i7 17 inches U...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>‚Çπ93,999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product             Ratings  \\\n",
       "0  LG Gram 14 inches Ultra-Light Intel Evo 11th G...  4.4 out of 5 stars   \n",
       "1  LG Gram Intel Evo 11th Gen Core i7 17 inches U...  4.5 out of 5 stars   \n",
       "2  HP Pavilion 14, 11th Gen Intel Core i7-16GB RA...  4.5 out of 5 stars   \n",
       "3  ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...  4.5 out of 5 stars   \n",
       "4  Lenovo ThinkBook 15 Intel 11th Gen Core i7 15....  4.6 out of 5 stars   \n",
       "5  LG Gram 16 inches Intel Evo 11th Gen Core i7 U...  4.4 out of 5 stars   \n",
       "6  Dell Alienware x17 R1 (2021) i7-11800H Gaming ...  4.0 out of 5 stars   \n",
       "7  HP Pavilion x360 11th Gen Intel Core i7 14 inc...  5.0 out of 5 stars   \n",
       "8  ASUS TUF Gaming F15 (2021) 15.6-inch (39.62 cm...  4.5 out of 5 stars   \n",
       "9  LG Gram Intel Evo 11th Gen Core i7 17 inches U...  4.0 out of 5 stars   \n",
       "\n",
       "       Price  \n",
       "0    ‚Çπ88,499  \n",
       "1    ‚Çπ93,999  \n",
       "2    ‚Çπ86,990  \n",
       "3    ‚Çπ57,490  \n",
       "4    ‚Çπ86,990  \n",
       "5    ‚Çπ89,611  \n",
       "6  ‚Çπ3,25,428  \n",
       "7    ‚Çπ86,990  \n",
       "8    ‚Çπ92,990  \n",
       "9    ‚Çπ93,999  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop_input=driver.find_element_by_xpath('/html/body/div[2]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "laptop_input.send_keys('Laptop')\n",
    "search_button = driver.find_element_by_xpath('/html/body/div[2]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_button.click()\n",
    "time.sleep(2)\n",
    "filter=driver.find_elements_by_xpath(\"//span[@class='a-size-base a-color-base']\")\n",
    "for i in filter:\n",
    "    if(i.text=='Intel Core i7'):\n",
    "        i.click()\n",
    "        break\n",
    "product = []\n",
    "rating_list = []\n",
    "Price = []\n",
    "product_title = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "for i in product_title:\n",
    "    product.append(i.text)\n",
    "# rating_tags = driver.find_elements_by_xpath('//div[@class=\"a-row a-size-small\"]/span[1]')\n",
    "# for i in rating_tags:\n",
    "#     rating_list.append(i.text)\n",
    "price_tag = driver.find_elements_by_xpath('//span[@class=\"a-price\"]')\n",
    "for i in price_tag:\n",
    "    Price.append(i.text)\n",
    "from bs4 import BeautifulSoup\n",
    "soup=BeautifulSoup(driver.page_source,'html.parser')\n",
    "results = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "item =results[0]\n",
    "results = soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "for item in results:\n",
    "    rcrd=extract_rating(item)\n",
    "    if rcrd:\n",
    "        rating_list.append(extract_rating(item))\n",
    "rating_records\n",
    "amazon_lappy_data = pd.DataFrame({})\n",
    "amazon_lappy_data['Product']=product[:10]\n",
    "amazon_lappy_data['Ratings'] = rating_list[:10]\n",
    "amazon_lappy_data['Price']=Price[:10]\n",
    "amazon_lappy_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7c43c",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of ‚ÄúSearch by Designations, Companies, Skills‚Äù enter\n",
    "‚ÄúData Scientist‚Äù and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of ‚ÄúSearch location‚Äù enter\n",
    "‚ÄúNoida‚Äù and select location ‚ÄúNoida‚Äù.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ca7fc44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver2 = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "time.sleep(3)\n",
    "#as if we go via 2nd step it is not giving search option so directly going via link\n",
    "driver2.get(\"https://www.ambitionbox.com/jobs/search?designation=data-scientist\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "158919ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_tag = driver2.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[1]/i')\n",
    "enter_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "86dce368",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver2.find_element_by_id(\"location_Noida\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7f6f1b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Job Posted</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tech Mahindra Ltd</td>\n",
       "      <td>18d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>21d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>25d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zyoin</td>\n",
       "      <td>3d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Newgen Software Technologies Ltd.</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JK Technosoft Ltd</td>\n",
       "      <td>14d ago</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Microsoft India (R and D) Pvt Ltd</td>\n",
       "      <td>2mon ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Paytm Payments Bank Limited</td>\n",
       "      <td>24d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Agreeya Solutions India Private Limited</td>\n",
       "      <td>28d ago</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Company Job Posted Rating\n",
       "0            GENPACT India Private Limited     4d ago    4.0\n",
       "1                        Tech Mahindra Ltd    18d ago    3.7\n",
       "2            GENPACT India Private Limited    21d ago    4.0\n",
       "3                         HCL Technologies    25d ago    3.8\n",
       "4                                    Zyoin     3d ago    4.1\n",
       "5        Newgen Software Technologies Ltd.     5d ago    3.5\n",
       "6                        JK Technosoft Ltd    14d ago    3.6\n",
       "7        Microsoft India (R and D) Pvt Ltd   2mon ago    4.3\n",
       "8              Paytm Payments Bank Limited    24d ago    3.7\n",
       "9  Agreeya Solutions India Private Limited    28d ago    3.3"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies=[]\n",
    "days_ago = []\n",
    "rating = []\n",
    "company_name_tag = driver2.find_elements_by_xpath('//p[@class=\"company body-medium\"]')\n",
    "for i in company_name_tag:\n",
    "    companies.append(i.text)\n",
    "days_ago_tag = driver2.find_elements_by_xpath('//span[@class=\"body-small-l\"][1]')\n",
    "for i in days_ago_tag:\n",
    "    days_ago.append(i.text)\n",
    "rating_tag = driver2.find_elements_by_xpath('//span[@class=\"body-small\"]')\n",
    "for i in rating_tag:\n",
    "    rating.append(i.text)\n",
    "driver2.close()\n",
    "Datascientist_noida_data = pd.DataFrame({})\n",
    "Datascientist_noida_data['Company']=companies\n",
    "Datascientist_noida_data['Job Posted'] = days_ago\n",
    "Datascientist_noida_data['Rating']=rating\n",
    "Datascientist_noida_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8d536962",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=r'C:/Users/Akhil/Desktop/roboflip/Web_Scraping_Selenium_Assg/chromedriver.exe', chrome_options=chrome_options)\n",
    "time.sleep(3)\n",
    "driver.get(\"https://www.ambitionbox.com/\")\n",
    "time.sleep(3)\n",
    "job = driver.find_element_by_xpath(\"//a[@class='link salaries']\").click()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath(\"//div[@class='searchbox-wrapper']/span/input\").send_keys('Data Scientist')\n",
    "time.sleep(3)\n",
    "\n",
    "#Extracting element of data scientist designation\n",
    "ds = driver.find_element_by_xpath('//*[@id=\"salaries\"]/main/section[1]/div[2]/div[1]/span/div/div/div[1]/div/div/p').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "322992b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>No of salaries based on</th>\n",
       "      <th>Average salary</th>\n",
       "      <th>Minimum salary</th>\n",
       "      <th>Maximum salary</th>\n",
       "      <th>Experience required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>based on 11 salaries</td>\n",
       "      <td>‚Çπ 29.7L</td>\n",
       "      <td>‚Çπ 25.0L</td>\n",
       "      <td>35.0L</td>\n",
       "      <td>3 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev</td>\n",
       "      <td>based on 31 salaries</td>\n",
       "      <td>‚Çπ 20.5L</td>\n",
       "      <td>‚Çπ 15.0L</td>\n",
       "      <td>25.5L</td>\n",
       "      <td>3-4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliance Jio</td>\n",
       "      <td>based on 10 salaries</td>\n",
       "      <td>‚Çπ 18.9L</td>\n",
       "      <td>‚Çπ 5.6L</td>\n",
       "      <td>26.2L</td>\n",
       "      <td>4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS</td>\n",
       "      <td>based on 15 salaries</td>\n",
       "      <td>‚Çπ 15.9L</td>\n",
       "      <td>‚Çπ 9.8L</td>\n",
       "      <td>20.0L</td>\n",
       "      <td>2 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optum</td>\n",
       "      <td>based on 28 salaries</td>\n",
       "      <td>‚Çπ 15.4L</td>\n",
       "      <td>‚Çπ 11.0L</td>\n",
       "      <td>22.4L</td>\n",
       "      <td>3-4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>based on 81 salaries</td>\n",
       "      <td>‚Çπ 15.1L</td>\n",
       "      <td>‚Çπ 9.5L</td>\n",
       "      <td>22.0L</td>\n",
       "      <td>2-4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics</td>\n",
       "      <td>based on 46 salaries</td>\n",
       "      <td>‚Çπ 14.8L</td>\n",
       "      <td>‚Çπ 9.0L</td>\n",
       "      <td>20.0L</td>\n",
       "      <td>2-4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>based on 53 salaries</td>\n",
       "      <td>‚Çπ 14.0L</td>\n",
       "      <td>‚Çπ 8.3L</td>\n",
       "      <td>20.5L</td>\n",
       "      <td>2-4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Veriz</td>\n",
       "      <td>based on 14 salaries</td>\n",
       "      <td>‚Çπ 12.7L</td>\n",
       "      <td>‚Çπ 10.0L</td>\n",
       "      <td>21.0L</td>\n",
       "      <td>4 yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ganit Business Solutis</td>\n",
       "      <td>based on 13 salaries</td>\n",
       "      <td>‚Çπ 12.4L</td>\n",
       "      <td>‚Çπ 8.5L</td>\n",
       "      <td>15.0L</td>\n",
       "      <td>4 yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Company No of salaries based on Average salary  \\\n",
       "0                 Walmart    based on 11 salaries        ‚Çπ 29.7L   \n",
       "1                Ab Inbev    based on 31 salaries        ‚Çπ 20.5L   \n",
       "2            Reliance Jio    based on 10 salaries        ‚Çπ 18.9L   \n",
       "3                      ZS    based on 15 salaries        ‚Çπ 15.9L   \n",
       "4                   Optum    based on 28 salaries        ‚Çπ 15.4L   \n",
       "5       Fractal Analytics    based on 81 salaries        ‚Çπ 15.1L   \n",
       "6         Tiger Analytics    based on 46 salaries        ‚Çπ 14.8L   \n",
       "7            UnitedHealth    based on 53 salaries        ‚Çπ 14.0L   \n",
       "8                   Veriz    based on 14 salaries        ‚Çπ 12.7L   \n",
       "9  Ganit Business Solutis    based on 13 salaries        ‚Çπ 12.4L   \n",
       "\n",
       "  Minimum salary Maximum salary Experience required  \n",
       "0        ‚Çπ 25.0L          35.0L               3 yrs  \n",
       "1        ‚Çπ 15.0L          25.5L             3-4 yrs  \n",
       "2         ‚Çπ 5.6L          26.2L               4 yrs  \n",
       "3         ‚Çπ 9.8L          20.0L               2 yrs  \n",
       "4        ‚Çπ 11.0L          22.4L             3-4 yrs  \n",
       "5         ‚Çπ 9.5L          22.0L             2-4 yrs  \n",
       "6         ‚Çπ 9.0L          20.0L             2-4 yrs  \n",
       "7         ‚Çπ 8.3L          20.5L             2-4 yrs  \n",
       "8        ‚Çπ 10.0L          21.0L               4 yrs  \n",
       "9         ‚Çπ 8.5L          15.0L               4 yrs  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting elements for company name\n",
    "company = driver.find_elements_by_xpath('//div[@class=\"name\"]')\n",
    "#creating an empty list individually\n",
    "name = []\n",
    "ts = [] \n",
    "avg_sal = [] \n",
    "min_sal=[]\n",
    "max_sal=[]\n",
    "exp=[] \n",
    "#Extracting all company names into a list\n",
    "for i in company[:10]:\n",
    "    ele1 = i.text.replace('based','').replace('on','').replace('salaries','').replace('\\n','').split()[:][0:-1]\n",
    "    string1 = ' '.join(ele1)\n",
    "    name.append(string1)\n",
    "#Extracting elements for total salary record\n",
    "total_salary_record = driver.find_elements_by_xpath('//div[@class=\"name\"]')\n",
    "for i in total_salary_record:\n",
    "    ele2 = i.text.split()[:][-4:]\n",
    "    string2 = ' '.join(ele2)\n",
    "    ts.append(string2)\n",
    "#Extracting elements for average salary\n",
    "average_salary = driver.find_elements_by_xpath('//p[@class=\"averageCtc\"]')\n",
    "for i in average_salary:\n",
    "    avg_sal.append(i.text)\n",
    "#Extracting elements for minimum salary\n",
    "minimum_salary = driver.find_elements_by_xpath('//div[@class=\"salary-values\"]')\n",
    "for i in minimum_salary:\n",
    "    ele3 = i.text.split()[::][0:2]\n",
    "    string3 = ' '.join(ele3)\n",
    "    min_sal.append(string3)\n",
    "#Extracting elements for maximum salary\n",
    "maximum_salary = driver.find_elements_by_xpath('//div[@class=\"salary-values\"]')\n",
    "\n",
    "for i in maximum_salary:\n",
    "    ele4 = i.text.split()[::][3:]\n",
    "    string4 = ' '.join(ele4)\n",
    "    max_sal.append(string4)\n",
    "#Extracting elements for experience required\n",
    "experience_required = driver.find_elements_by_xpath('//div[@class=\"salaries sbold-list-header\"]')\n",
    "\n",
    "for i in experience_required:\n",
    "    ele5 = i.text.replace('Data Scientist\\n','').replace('\\n','').replace('exp','').split()[::][1:]\n",
    "    string5 = ' '.join(ele5)\n",
    "    exp.append(string5)\n",
    "salary_data = pd.DataFrame({'Company':name,'No of salaries based on':ts,'Average salary':avg_sal,'Minimum salary':min_sal,'Maximum salary':max_sal,'Experience required':exp})\n",
    "salary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444471f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
